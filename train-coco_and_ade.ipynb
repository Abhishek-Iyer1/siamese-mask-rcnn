{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:98% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "#%load_ext line_profiler\n",
    "\n",
    "import tensorflow as tf\n",
    "slim = tf.contrib.slim\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "sess_config = tf.ConfigProto()\n",
    "#config.gpu_options.allow_growth=True\n",
    "#config.gpu_options.per_process_gpu_memory_fraction=0.45\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "ADE20K_DATA = '/gpfs01/bethge/data/ADE20K_2016_07_26'\n",
    "COCO_DATA = '/gpfs01/bethge/share/mscoco/COCO'\n",
    "PASCAL_VOC_DATA = '/gpfs01/bethge/data/PascalVOC'\n",
    "MASK_RCNN_MODEL_PATH = 'Mask_RCNN/'\n",
    "SLIM_MODELS_PATH = 'slim/'\n",
    "TRANSFORMER_MODELS_PATH = 'transformer/'\n",
    "SIAMESE_MASK_RCNN_PATH = '/gpfs01/bethge/home/cmichaelis/projects/2018-03_Siamese_Mask_RCNN/siamese-mask-rcnn/'\n",
    "\n",
    "if MASK_RCNN_MODEL_PATH not in sys.path:\n",
    "    sys.path.append(MASK_RCNN_MODEL_PATH)\n",
    "if SIAMESE_MASK_RCNN_PATH not in sys.path:\n",
    "    sys.path.append(SIAMESE_MASK_RCNN_PATH)\n",
    "    \n",
    "from samples.coco import coco\n",
    "from mrcnn import utils\n",
    "from mrcnn import model as modellib\n",
    "from mrcnn import visualize\n",
    "    \n",
    "if SLIM_MODELS_PATH not in sys.path:\n",
    "    sys.path.append(SLIM_MODELS_PATH)\n",
    "if TRANSFORMER_MODELS_PATH not in sys.path:\n",
    "    sys.path.append(TRANSFORMER_MODELS_PATH)\n",
    "    \n",
    "import utils as siamese_utils\n",
    "import model as siamese_model\n",
    "import siamese_mrcnn_models as model_zoo\n",
    "    \n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "import imgaug\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "from spatial_transformer import transformer\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:98% !important; }</style>\"))\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.getcwd()\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_nopascal_classes = [8,10,11,12,13,14,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,60,62,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80]\n",
    "coco_pascal_classes = np.array(range(1,81))[np.array([i not in coco_nopascal_classes for i in range(1,81)])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pascal_ade20k.pkl', 'rb') as f:\n",
    "    pascal_ade20k = pickle.load(f)\n",
    "\n",
    "ade20k_pascal_classes = np.array(sorted(list(set(sum([l for l in pascal_ade20k.values()], [])))))\n",
    "ade20k_nopascal_classes = np.setdiff1d(range(1, 3149), ade20k_pascal_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet50\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     6\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        15\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.5\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "FPN_FEATUREMAPS                128\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 6\n",
      "IMAGE_MAX_DIM                  512\n",
      "IMAGE_META_SIZE                3161\n",
      "IMAGE_MIN_DIM                  400\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [512 512   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 2.0, 'rpn_bbox_loss': 0.2, 'mrcnn_class_loss': 2.0, 'mrcnn_bbox_loss': 0.5, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               15\n",
      "MAX_TARGET_INSTANCES           10\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           coco\n",
      "NUM_CLASSES                    3149\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        250\n",
      "POST_NMS_ROIS_TRAINING         250\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
      "RPN_ANCHOR_STRIDE              2\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    64\n",
      "STEPS_PER_EPOCH                1000\n",
      "TARGET_MAX_DIM                 96\n",
      "TARGET_MIN_DIM                 75\n",
      "TARGET_PADDING                 True\n",
      "TARGET_SHAPE                   [96 96  3]\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           50\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class TrainConfig(coco.CocoConfig):\n",
    "    # Set batch size to 1 since we'll be running inference on\n",
    "    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 6\n",
    "    LEARNING_RATE = 0.001\n",
    "    NUM_CLASSES = 3148 + 1\n",
    "    TARGET_MAX_DIM = 96\n",
    "    TARGET_MIN_DIM = 75\n",
    "    IMAGE_MIN_DIM = 400\n",
    "    IMAGE_MAX_DIM = 512\n",
    "    #IMAGE_RESIZE_MODE = 'none'\n",
    "    TARGET_SHAPE = np.array([TARGET_MAX_DIM, TARGET_MAX_DIM, 3])\n",
    "    TARGET_PADDING = True\n",
    "    MAX_TARGET_INSTANCES = 10\n",
    "    # Reduce model size for prototyping\n",
    "    BACKBONE = 'resnet50'\n",
    "    FPN_FEATUREMAPS = 128\n",
    "    RPN_ANCHOR_STRIDE = 2\n",
    "    RPN_TRAIN_ANCHORS_PER_IMAGE = 64\n",
    "    POST_NMS_ROIS_TRAINING = 250\n",
    "    POST_NMS_ROIS_INFERENCE = 250\n",
    "    TRAIN_ROIS_PER_IMAGE = 50\n",
    "    DETECTION_MAX_INSTANCES = 15\n",
    "    DETECTION_NMS_THRESHOLD = 0.5\n",
    "    MAX_GT_INSTANCES = 15\n",
    "    LOSS_WEIGHTS = {'rpn_class_loss': 2.0, \n",
    "                    'rpn_bbox_loss': 0.2, \n",
    "                    'mrcnn_class_loss': 2.0, \n",
    "                    'mrcnn_bbox_loss': 0.5, \n",
    "                    'mrcnn_mask_loss': 1.0}\n",
    "\n",
    "config = TrainConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=21.25s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=2.17s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# Load COCO/train dataset\n",
    "coco_train = siamese_utils.IndexedCocoDataset()\n",
    "coco_train.set_active_classes(coco_nopascal_classes)\n",
    "coco_train.load_coco(COCO_DATA, \"train\", year=\"2017\")\n",
    "coco_train.prepare()\n",
    "coco_train.build_indices()\n",
    "coco_train.ACTIVE_CLASSES = np.array(range(1,81))\n",
    "\n",
    "# Load COCO/val dataset\n",
    "coco_val = siamese_utils.IndexedCocoDataset()\n",
    "coco_val.set_active_classes(coco_nopascal_classes)\n",
    "coco_val.load_coco(COCO_DATA, \"val\", year=\"2017\")\n",
    "coco_val.prepare()\n",
    "coco_val.build_indices()\n",
    "coco_val.ACTIVE_CLASSES = np.array(range(1,81))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ADE20K/train dataset\n",
    "ade20k_train = siamese_utils.IndexedADE20KDataset()\n",
    "ade20k_train.set_active_classes(ade20k_nopascal_classes)\n",
    "ade20k_train.load_ade20k(ADE20K_DATA, \"train\")\n",
    "ade20k_train.prepare()\n",
    "ade20k_train.build_indices()\n",
    "ade20k_train.ACTIVE_CLASSES = np.array(range(1,3148))\n",
    "\n",
    "# Load ADE20K/val dataset\n",
    "ade20k_val = siamese_utils.IndexedADE20KDataset()\n",
    "ade20k_val.set_active_classes(ade20k_nopascal_classes)\n",
    "ade20k_val.load_ade20k(ADE20K_DATA, \"val\")\n",
    "ade20k_val.prepare()\n",
    "ade20k_val.build_indices()\n",
    "ade20k_val.ACTIVE_CLASSES = np.array(range(1,3148))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def siamese_data_generator(datasets, config, shuffle=True, augmentation=imgaug.augmenters.Fliplr(0.5), random_rois=0,\n",
    "                   batch_size=1, detection_targets=False, diverse=0):\n",
    "    \"\"\"A generator that returns images and corresponding target class ids,\n",
    "    bounding box deltas, and masks.\n",
    "    dataset: The Dataset object to pick data from\n",
    "    config: The model config object\n",
    "    shuffle: If True, shuffles the samples before every epoch\n",
    "    augment: If True, applies image augmentation to images (currently only\n",
    "             horizontal flips are supported)\n",
    "    random_rois: If > 0 then generate proposals to be used to train the\n",
    "                 network classifier and mask heads. Useful if training\n",
    "                 the Mask RCNN part without the RPN.\n",
    "    batch_size: How many images to return in each call\n",
    "    detection_targets: If True, generate detection targets (class IDs, bbox\n",
    "        deltas, and masks). Typically for debugging or visualizations because\n",
    "        in trainig detection targets are generated by DetectionTargetLayer.\n",
    "    diverse: Float in [0,1] indicatiing probability to draw a target\n",
    "        from any random class instead of one from the image classes\n",
    "    Returns a Python generator. Upon calling next() on it, the\n",
    "    generator returns two lists, inputs and outputs. The containtes\n",
    "    of the lists differs depending on the received arguments:\n",
    "    inputs list:\n",
    "    - images: [batch, H, W, C]\n",
    "    - image_meta: [batch, size of image meta]\n",
    "    - rpn_match: [batch, N] Integer (1=positive anchor, -1=negative, 0=neutral)\n",
    "    - rpn_bbox: [batch, N, (dy, dx, log(dh), log(dw))] Anchor bbox deltas.\n",
    "    - gt_class_ids: [batch, MAX_GT_INSTANCES] Integer class IDs\n",
    "    - gt_boxes: [batch, MAX_GT_INSTANCES, (y1, x1, y2, x2)]\n",
    "    - gt_masks: [batch, height, width, MAX_GT_INSTANCES]. The height and width\n",
    "                are those of the image unless use_mini_mask is True, in which\n",
    "                case they are defined in MINI_MASK_SHAPE.\n",
    "    outputs list: Usually empty in regular training. But if detection_targets\n",
    "        is True then the outputs list contains target class_ids, bbox deltas,\n",
    "        and masks.\n",
    "    \"\"\"\n",
    "    b = 0  # batch item index\n",
    "    error_count = 0\n",
    "    \n",
    "    number_of_datsets = len(datasets)\n",
    "    image_index = []\n",
    "    image_ids = []\n",
    "    for dataset in datasets:\n",
    "        image_ids.append(np.copy(dataset.image_ids))\n",
    "        image_index.append(-1)\n",
    "\n",
    "    # Anchors\n",
    "    # [anchor_count, (y1, x1, y2, x2)]\n",
    "    backbone_shapes = modellib.compute_backbone_shapes(config, config.IMAGE_SHAPE)\n",
    "    anchors = utils.generate_pyramid_anchors(config.RPN_ANCHOR_SCALES,\n",
    "                                             config.RPN_ANCHOR_RATIOS,\n",
    "                                             backbone_shapes,\n",
    "                                             config.BACKBONE_STRIDES,\n",
    "                                             config.RPN_ANCHOR_STRIDE)\n",
    "\n",
    "    # Keras requires a generator to run indefinately.\n",
    "    while True:\n",
    "        try:\n",
    "            dataset_id = np.random.choice(range(number_of_datsets))\n",
    "            dataset = datasets[dataset_id]\n",
    "            \n",
    "            # Increment index to pick next image. Shuffle if at the start of an epoch.\n",
    "            image_index[dataset_id] = (image_index[dataset_id] + 1) % len(image_ids[dataset_id])\n",
    "            if shuffle and image_index[dataset_id] == 0:\n",
    "                np.random.shuffle(image_ids[dataset_id])\n",
    "\n",
    "            # Get GT bounding boxes and masks for image.\n",
    "            image_id = image_ids[dataset_id][image_index[dataset_id]]\n",
    "            image, image_meta, gt_class_ids, gt_boxes, gt_masks = \\\n",
    "                modellib.load_image_gt(dataset, config, image_id, augmentation=augmentation,\n",
    "                              use_mini_mask=config.USE_MINI_MASK)\n",
    "\n",
    "            # Replace class ids with foreground/background info if binary\n",
    "            # class option is chosen\n",
    "            # if binary_classes == True:\n",
    "            #    gt_class_ids = np.minimum(gt_class_ids, 1)\n",
    "\n",
    "            # Skip images that have no instances. This can happen in cases\n",
    "            # where we train on a subset of classes and the image doesn't\n",
    "            # have any of the classes we care about.\n",
    "            if not np.any(gt_class_ids > 0):\n",
    "                continue\n",
    "                \n",
    "#             print(gt_class_ids)\n",
    "\n",
    "            # Use only positive class_ids\n",
    "            categories = np.unique(gt_class_ids)\n",
    "            _idx = categories > 0\n",
    "            categories = categories[_idx]\n",
    "            # Use only active classes\n",
    "            active_categories = []\n",
    "            for c in categories:\n",
    "                if any(c == dataset.ACTIVE_CLASSES):\n",
    "                    active_categories.append(c)\n",
    "            \n",
    "            # Skiop image if it contains no instance of any active class    \n",
    "            if not np.any(np.array(active_categories) > 0):\n",
    "                continue\n",
    "            # Randomly select category\n",
    "            category = np.random.choice(active_categories)\n",
    "                \n",
    "            # Generate siamese target crop\n",
    "            target = siamese_utils.get_one_target(category, dataset, config, augmentation=augmentation)\n",
    "            if target is None: # fix until a better ADE20K metadata is built\n",
    "                print('skip target')\n",
    "                continue\n",
    "#             print(target_class_id)\n",
    "            target_class_id = category\n",
    "            target_class_ids = np.array([target_class_id])\n",
    "            \n",
    "            idx = gt_class_ids == target_class_id\n",
    "            siamese_class_ids = idx.astype('int8')\n",
    "#             print(idx)\n",
    "#             print(gt_boxes.shape, gt_masks.shape)\n",
    "            siamese_class_ids = siamese_class_ids[idx]\n",
    "            gt_class_ids = gt_class_ids[idx]\n",
    "            gt_boxes = gt_boxes[idx,:]\n",
    "            gt_masks = gt_masks[:,:,idx]\n",
    "#             print(gt_boxes.shape, gt_masks.shape)\n",
    "\n",
    "            # RPN Targets\n",
    "            rpn_match, rpn_bbox = modellib.build_rpn_targets(image.shape, anchors,\n",
    "                                                    gt_class_ids, gt_boxes, config)\n",
    "\n",
    "            # Mask R-CNN Targets\n",
    "            if random_rois:\n",
    "                rpn_rois = modellib.generate_random_rois(\n",
    "                    image.shape, random_rois, gt_class_ids, gt_boxes)\n",
    "                if detection_targets:\n",
    "                    rois, mrcnn_class_ids, mrcnn_bbox, mrcnn_mask =\\\n",
    "                        modellib.build_detection_targets(\n",
    "                            rpn_rois, gt_class_ids, gt_boxes, gt_masks, config)\n",
    "\n",
    "            # Init batch arrays\n",
    "            if b == 0:\n",
    "                batch_image_meta = np.zeros(\n",
    "                    (batch_size, 3148+13), dtype=image_meta.dtype)\n",
    "                batch_rpn_match = np.zeros(\n",
    "                    [batch_size, anchors.shape[0], 1], dtype=rpn_match.dtype)\n",
    "                batch_rpn_bbox = np.zeros(\n",
    "                    [batch_size, config.RPN_TRAIN_ANCHORS_PER_IMAGE, 4], dtype=rpn_bbox.dtype)\n",
    "                batch_images = np.zeros(\n",
    "                    (batch_size,) + image.shape, dtype=np.float32)\n",
    "                batch_gt_class_ids = np.zeros(\n",
    "                    (batch_size, config.MAX_GT_INSTANCES), dtype=np.int32)\n",
    "                batch_gt_boxes = np.zeros(\n",
    "                    (batch_size, config.MAX_GT_INSTANCES, 4), dtype=np.int32)\n",
    "                batch_targets = np.zeros(\n",
    "                    (batch_size,) + target.shape, dtype=np.float32)\n",
    "#                 batch_target_class_ids = np.zeros(\n",
    "#                     (batch_size, config.MAX_TARGET_INSTANCES), dtype=np.int32)\n",
    "                if config.USE_MINI_MASK:\n",
    "                    batch_gt_masks = np.zeros((batch_size, config.MINI_MASK_SHAPE[0], config.MINI_MASK_SHAPE[1],\n",
    "                                               config.MAX_GT_INSTANCES))\n",
    "                else:\n",
    "                    batch_gt_masks = np.zeros(\n",
    "                        (batch_size, image.shape[0], image.shape[1], config.MAX_GT_INSTANCES))\n",
    "                if random_rois:\n",
    "                    batch_rpn_rois = np.zeros(\n",
    "                        (batch_size, rpn_rois.shape[0], 4), dtype=rpn_rois.dtype)\n",
    "                    if detection_targets:\n",
    "                        batch_rois = np.zeros(\n",
    "                            (batch_size,) + rois.shape, dtype=rois.dtype)\n",
    "                        batch_mrcnn_class_ids = np.zeros(\n",
    "                            (batch_size,) + mrcnn_class_ids.shape, dtype=mrcnn_class_ids.dtype)\n",
    "                        batch_mrcnn_bbox = np.zeros(\n",
    "                            (batch_size,) + mrcnn_bbox.shape, dtype=mrcnn_bbox.dtype)\n",
    "                        batch_mrcnn_mask = np.zeros(\n",
    "                            (batch_size,) + mrcnn_mask.shape, dtype=mrcnn_mask.dtype)\n",
    "\n",
    "            # If more instances than fits in the array, sub-sample from them.\n",
    "            if gt_boxes.shape[0] > config.MAX_GT_INSTANCES:\n",
    "                ids = np.random.choice(\n",
    "                    np.arange(gt_boxes.shape[0]), config.MAX_GT_INSTANCES, replace=False)\n",
    "                gt_class_ids = gt_class_ids[ids]\n",
    "                siamese_class_ids = siamese_class_ids[ids]\n",
    "                gt_boxes = gt_boxes[ids]\n",
    "                gt_masks = gt_masks[:, :, ids]\n",
    "                \n",
    "\n",
    "            # Add to batch\n",
    "            batch_image_meta[b][:image_meta.shape[0]] = image_meta\n",
    "            batch_rpn_match[b] = rpn_match[:, np.newaxis]\n",
    "            batch_rpn_bbox[b] = rpn_bbox\n",
    "            batch_images[b] = modellib.mold_image(image.astype(np.float32), config)\n",
    "            batch_targets[b] = modellib.mold_image(target.astype(np.float32), config)\n",
    "            batch_gt_class_ids[b, :siamese_class_ids.shape[0]] = siamese_class_ids\n",
    "#             batch_target_class_ids[b, :target_class_ids.shape[0]] = target_class_ids\n",
    "            batch_gt_boxes[b, :gt_boxes.shape[0]] = gt_boxes\n",
    "            batch_gt_masks[b, :, :, :gt_masks.shape[-1]] = gt_masks\n",
    "            if random_rois:\n",
    "                batch_rpn_rois[b] = rpn_rois\n",
    "                if detection_targets:\n",
    "                    batch_rois[b] = rois\n",
    "                    batch_mrcnn_class_ids[b] = mrcnn_class_ids\n",
    "                    batch_mrcnn_bbox[b] = mrcnn_bbox\n",
    "                    batch_mrcnn_mask[b] = mrcnn_mask\n",
    "            b += 1\n",
    "\n",
    "            # Batch full?\n",
    "            if b >= batch_size:\n",
    "                inputs = [batch_images, batch_image_meta, batch_targets, batch_rpn_match, batch_rpn_bbox,\n",
    "                          batch_gt_class_ids, batch_gt_boxes, batch_gt_masks]\n",
    "                outputs = []\n",
    "\n",
    "                if random_rois:\n",
    "                    inputs.extend([batch_rpn_rois])\n",
    "                    if detection_targets:\n",
    "                        inputs.extend([batch_rois])\n",
    "                        # Keras requires that output and targets have the same number of dimensions\n",
    "                        batch_mrcnn_class_ids = np.expand_dims(\n",
    "                            batch_mrcnn_class_ids, -1)\n",
    "                        outputs.extend(\n",
    "                            [batch_mrcnn_class_ids, batch_mrcnn_bbox, batch_mrcnn_mask])\n",
    "\n",
    "                yield inputs, outputs\n",
    "\n",
    "                # start a new batch\n",
    "                b = 0\n",
    "        except (GeneratorExit, KeyboardInterrupt):\n",
    "            raise\n",
    "        except:\n",
    "            # Log it and skip the image\n",
    "            modellib.logging.exception(\"Error processing image {}\".format(\n",
    "                dataset.image_info[image_id]))\n",
    "            error_count += 1\n",
    "            if error_count > 5:\n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_utils.siamese_data_generator = siamese_data_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "import keras.layers as KL\n",
    "import keras.initializers as KI\n",
    "import keras.engine as KE\n",
    "import keras.models as KM\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewSiameseMaskRCNN(model_zoo.NewSiameseMaskRCNN):\n",
    "    \"\"\"Encapsulates the Mask RCNN model functionality.\n",
    "    The actual Keras model is in the keras_model property.\n",
    "    \"\"\"\n",
    "\n",
    "    def build(self, mode, config):\n",
    "        \"\"\"Build Mask R-CNN architecture.\n",
    "            input_shape: The shape of the input image.\n",
    "            mode: Either \"training\" or \"inference\". The inputs and\n",
    "                outputs of the model differ accordingly.\n",
    "        \"\"\"\n",
    "        assert mode in ['training', 'inference']\n",
    "\n",
    "        # Image size must be dividable by 2 multiple times\n",
    "        h, w = config.IMAGE_SHAPE[:2]\n",
    "        if h / 2**6 != int(h / 2**6) or w / 2**6 != int(w / 2**6):\n",
    "            raise Exception(\"Image size must be dividable by 2 at least 6 times \"\n",
    "                            \"to avoid fractions when downscaling and upscaling.\"\n",
    "                            \"For example, use 256, 320, 384, 448, 512, ... etc. \")\n",
    "\n",
    "        # Inputs\n",
    "        input_image = KL.Input(\n",
    "            shape=config.IMAGE_SHAPE.tolist(), name=\"input_image\")\n",
    "        # CHANGE: add target input\n",
    "        input_target = KL.Input(\n",
    "            shape=config.TARGET_SHAPE.tolist(), name=\"input_target\")\n",
    "        input_image_meta = KL.Input(shape=[config.IMAGE_META_SIZE],\n",
    "                                    name=\"input_image_meta\")\n",
    "        if mode == \"training\":\n",
    "            # RPN GT\n",
    "            input_rpn_match = KL.Input(\n",
    "                shape=[None, 1], name=\"input_rpn_match\", dtype=tf.int32)\n",
    "            input_rpn_bbox = KL.Input(\n",
    "                shape=[None, 4], name=\"input_rpn_bbox\", dtype=tf.float32)\n",
    "\n",
    "            # Detection GT (class IDs, bounding boxes, and masks)\n",
    "            # 1. GT Class IDs (zero padded)\n",
    "            input_gt_class_ids = KL.Input(\n",
    "                shape=[None], name=\"input_gt_class_ids\", dtype=tf.int32)\n",
    "            # 2. GT Boxes in pixels (zero padded)\n",
    "            # [batch, MAX_GT_INSTANCES, (y1, x1, y2, x2)] in image coordinates\n",
    "            input_gt_boxes = KL.Input(\n",
    "                shape=[None, 4], name=\"input_gt_boxes\", dtype=tf.float32)\n",
    "            # Normalize coordinates\n",
    "            gt_boxes = KL.Lambda(lambda x: modellib.norm_boxes_graph(\n",
    "                x, K.shape(input_image)[1:3]))(input_gt_boxes)\n",
    "            # 3. GT Masks (zero padded)\n",
    "            # [batch, height, width, MAX_GT_INSTANCES]\n",
    "            if config.USE_MINI_MASK:\n",
    "                input_gt_masks = KL.Input(\n",
    "                    shape=[config.MINI_MASK_SHAPE[0],\n",
    "                           config.MINI_MASK_SHAPE[1], None],\n",
    "                    name=\"input_gt_masks\", dtype=bool)\n",
    "            else:\n",
    "                input_gt_masks = KL.Input(\n",
    "                    shape=[config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1], None],\n",
    "                    name=\"input_gt_masks\", dtype=bool)\n",
    "        elif mode == \"inference\":\n",
    "            # Anchors in normalized coordinates\n",
    "            input_anchors = KL.Input(shape=[None, 4], name=\"input_anchors\")\n",
    "\n",
    "        # Build the shared convolutional layers.\n",
    "        # CHANGE: Use weightshared FPN model for image and target\n",
    "        # Create FPN Model\n",
    "        resnet = siamese_model.build_resnet_model(self.config)\n",
    "        fpn = model_zoo.build_new_fpn_model(feature_maps=self.config.FPN_FEATUREMAPS)\n",
    "        siamese_distance = model_zoo.build_siamese_distance_model(feature_maps=self.config.FPN_FEATUREMAPS//32)\n",
    "        # Create Image FP\n",
    "        _, IC2, IC3, IC4, IC5 = resnet(input_image)\n",
    "        IP2, IP3, IP4, IP5, IP6 = fpn([IC2, IC3, IC4, IC5])\n",
    "        # Create Target FR\n",
    "        _, TC2, TC3, TC4, TC5 = resnet(input_target)\n",
    "        TP2, TP3, TP4, TP5, TP6 = fpn([TC2, TC3, TC4, TC5])\n",
    "        \n",
    "        # CHANGE: add siamese distance copmputation\n",
    "        # Combine FPs using L1 distance\n",
    "        DP2 = siamese_distance([IP2, TP2, TP3, TP4, TP5, TP6])\n",
    "        DP3 = siamese_distance([IP3, TP2, TP3, TP4, TP5, TP6])\n",
    "        DP4 = siamese_distance([IP4, TP2, TP3, TP4, TP5, TP6])\n",
    "        DP5 = siamese_distance([IP5, TP2, TP3, TP4, TP5, TP6])\n",
    "        DP6 = siamese_distance([IP6, TP2, TP3, TP4, TP5, TP6])\n",
    "        \n",
    "        # CHANGE: combine original and siamese features\n",
    "        P2 = KL.Concatenate()([IP2, DP2])\n",
    "        P3 = KL.Concatenate()([IP3, DP3])\n",
    "        P4 = KL.Concatenate()([IP4, DP4])\n",
    "        P5 = KL.Concatenate()([IP5, DP5])\n",
    "        P6 = KL.Concatenate()([IP6, DP6])\n",
    "\n",
    "        # Note that P6 is used in RPN, but not in the classifier heads.\n",
    "        rpn_feature_maps = [P2, P3, P4, P5, P6]\n",
    "        mrcnn_feature_maps = [P2, P3, P4, P5]\n",
    "\n",
    "        # Anchors\n",
    "        if mode == \"training\":\n",
    "            anchors = self.get_anchors(config.IMAGE_SHAPE)\n",
    "            # Duplicate across the batch dimension because Keras requires it\n",
    "            # TODO: can this be optimized to avoid duplicating the anchors?\n",
    "            anchors = np.broadcast_to(anchors, (config.BATCH_SIZE,) + anchors.shape)\n",
    "            # A hack to get around Keras's bad support for constants\n",
    "            anchors = KL.Lambda(lambda x: tf.Variable(anchors), name=\"anchors\")(input_image)\n",
    "        else:\n",
    "            anchors = input_anchors\n",
    "\n",
    "        # RPN Model\n",
    "        # CHANGE: Set number of filters to 208 [128 original + 5*4*4 L1 + CC]\n",
    "        rpn = modellib.build_rpn_model(config.RPN_ANCHOR_STRIDE,\n",
    "                              len(config.RPN_ANCHOR_RATIOS), (self.config.FPN_FEATUREMAPS + 5*4*4))\n",
    "        # Loop through pyramid layers\n",
    "        layer_outputs = []  # list of lists\n",
    "        for p in rpn_feature_maps:\n",
    "            layer_outputs.append(rpn([p]))\n",
    "        # Concatenate layer outputs\n",
    "        # Convert from list of lists of level outputs to list of lists\n",
    "        # of outputs across levels.\n",
    "        # e.g. [[a1, b1, c1], [a2, b2, c2]] => [[a1, a2], [b1, b2], [c1, c2]]\n",
    "        output_names = [\"rpn_class_logits\", \"rpn_class\", \"rpn_bbox\"]\n",
    "        outputs = list(zip(*layer_outputs))\n",
    "        outputs = [KL.Concatenate(axis=1, name=n)(list(o))\n",
    "                   for o, n in zip(outputs, output_names)]\n",
    "\n",
    "        rpn_class_logits, rpn_class, rpn_bbox = outputs\n",
    "\n",
    "        # Generate proposals\n",
    "        # Proposals are [batch, N, (y1, x1, y2, x2)] in normalized coordinates\n",
    "        # and zero padded.\n",
    "        proposal_count = config.POST_NMS_ROIS_TRAINING if mode == \"training\"\\\n",
    "            else config.POST_NMS_ROIS_INFERENCE\n",
    "        rpn_rois = modellib.ProposalLayer(\n",
    "            proposal_count=proposal_count,\n",
    "            nms_threshold=config.RPN_NMS_THRESHOLD,\n",
    "            name=\"ROI\",\n",
    "            config=config)([rpn_class, rpn_bbox, anchors])\n",
    "\n",
    "        if mode == \"training\":\n",
    "            # Class ID mask to mark class IDs supported by the dataset the image\n",
    "            # came from.\n",
    "            active_class_ids = KL.Lambda(\n",
    "                lambda x: modellib.parse_image_meta_graph(x)[\"active_class_ids\"]\n",
    "                )(input_image_meta)\n",
    "\n",
    "            if not config.USE_RPN_ROIS:\n",
    "                # Ignore predicted ROIs and use ROIs provided as an input.\n",
    "                input_rois = KL.Input(shape=[config.POST_NMS_ROIS_TRAINING, 4],\n",
    "                                      name=\"input_roi\", dtype=np.int32)\n",
    "                # Normalize coordinates\n",
    "                target_rois = KL.Lambda(lambda x: modellig.norm_boxes_graph(\n",
    "                    x, K.shape(input_image)[1:3]))(input_rois)\n",
    "            else:\n",
    "                target_rois = rpn_rois\n",
    "\n",
    "            # Generate detection targets\n",
    "            # Subsamples proposals and generates target outputs for training\n",
    "            # Note that proposal class IDs, gt_boxes, and gt_masks are zero\n",
    "            # padded. Equally, returned rois and targets are zero padded.\n",
    "            rois, target_class_ids, target_bbox, target_mask =\\\n",
    "                modellib.DetectionTargetLayer(config, name=\"proposal_targets\")([\n",
    "                    target_rois, input_gt_class_ids, gt_boxes, input_gt_masks])\n",
    "\n",
    "            # Network Heads\n",
    "            # TODO: verify that this handles zero padded ROIs\n",
    "            # CHANGE: reduce number of classes to 2\n",
    "            mrcnn_class_logits, mrcnn_class, mrcnn_bbox =\\\n",
    "                model_zoo.new_fpn_classifier_graph(rois, mrcnn_feature_maps, input_image_meta,\n",
    "                                     config.POOL_SIZE, num_classes=2,\n",
    "                                     train_bn=config.TRAIN_BN)\n",
    "            # CHANGE: reduce number of classes to 2\n",
    "            mrcnn_mask = model_zoo.new_fpn_mask_graph(rois, mrcnn_feature_maps,\n",
    "                                              input_image_meta,\n",
    "                                              config.MASK_POOL_SIZE,\n",
    "                                              num_classes=2,\n",
    "                                              train_bn=config.TRAIN_BN)\n",
    "\n",
    "            # TODO: clean up (use tf.identify if necessary)\n",
    "            output_rois = KL.Lambda(lambda x: x * 1, name=\"output_rois\")(rois)\n",
    "\n",
    "            # Losses\n",
    "            rpn_class_loss = KL.Lambda(lambda x: modellib.rpn_class_loss_graph(*x), name=\"rpn_class_loss\")(\n",
    "                [input_rpn_match, rpn_class_logits])\n",
    "            rpn_bbox_loss = KL.Lambda(lambda x: modellib.rpn_bbox_loss_graph(config, *x), name=\"rpn_bbox_loss\")(\n",
    "                [input_rpn_bbox, input_rpn_match, rpn_bbox])\n",
    "            # CHANGE: use custom class loss without using active_class_ids\n",
    "            class_loss = KL.Lambda(lambda x: siamese_model.mrcnn_class_loss_graph(*x), name=\"mrcnn_class_loss\")(\n",
    "                [target_class_ids, mrcnn_class_logits, active_class_ids])\n",
    "            bbox_loss = KL.Lambda(lambda x: modellib.mrcnn_bbox_loss_graph(*x), name=\"mrcnn_bbox_loss\")(\n",
    "                [target_bbox, target_class_ids, mrcnn_bbox])\n",
    "            mask_loss = KL.Lambda(lambda x: modellib.mrcnn_mask_loss_graph(*x), name=\"mrcnn_mask_loss\")(\n",
    "                [target_mask, target_class_ids, mrcnn_mask])\n",
    "\n",
    "            # Model\n",
    "            # CHANGE: Added target to inputs\n",
    "            inputs = [input_image, input_image_meta, input_target,\n",
    "                      input_rpn_match, input_rpn_bbox, input_gt_class_ids, input_gt_boxes, input_gt_masks]\n",
    "            if not config.USE_RPN_ROIS:\n",
    "                inputs.append(input_rois)\n",
    "            outputs = [rpn_class_logits, rpn_class, rpn_bbox,\n",
    "                       mrcnn_class_logits, mrcnn_class, mrcnn_bbox, mrcnn_mask,\n",
    "                       rpn_rois, output_rois,\n",
    "                       rpn_class_loss, rpn_bbox_loss, class_loss, bbox_loss, mask_loss]\n",
    "            model = KM.Model(inputs, outputs, name='mask_rcnn')\n",
    "        else:\n",
    "            # Network Heads\n",
    "            # Proposal classifier and BBox regressor heads\n",
    "            # CHANGE: reduce number of classes to 2\n",
    "            mrcnn_class_logits, mrcnn_class, mrcnn_bbox =\\\n",
    "                fpn_classifier_graph(rpn_rois, mrcnn_feature_maps, input_image_meta,\n",
    "                                     config.POOL_SIZE, num_classes=2,\n",
    "                                     train_bn=config.TRAIN_BN)\n",
    "\n",
    "            # Detections\n",
    "            # output is [batch, num_detections, (y1, x1, y2, x2, class_id, score)] in \n",
    "            # normalized coordinates\n",
    "            detections = modellib.DetectionLayer(config, name=\"mrcnn_detection\")(\n",
    "                [rpn_rois, mrcnn_class, mrcnn_bbox, input_image_meta])\n",
    "\n",
    "            # Create masks for detections\n",
    "            detection_boxes = KL.Lambda(lambda x: x[..., :4])(detections)\n",
    "            # CHANGE: reduce number of classes to 2\n",
    "            mrcnn_mask = build_fpn_mask_graph(detection_boxes, mrcnn_feature_maps,\n",
    "                                              input_image_meta,\n",
    "                                              config.MASK_POOL_SIZE,\n",
    "                                              num_classes=2,\n",
    "                                              train_bn=config.TRAIN_BN)\n",
    "\n",
    "            # CHANGE: Added target to the input\n",
    "            model = KM.Model([input_image, input_image_meta, input_target, input_anchors],\n",
    "                             [detections, mrcnn_class, mrcnn_bbox,\n",
    "                                 mrcnn_mask, rpn_rois, rpn_class, rpn_bbox],\n",
    "                             name='mask_rcnn')\n",
    "\n",
    "        # Add multi-GPU support.\n",
    "        if config.GPU_COUNT > 1:\n",
    "            from mrcnn.parallel_model import ParallelModel\n",
    "            model = ParallelModel(model, config.GPU_COUNT)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model object in inference mode.\n",
    "model = NewSiameseMaskRCNN(mode=\"training\", model_dir=MODEL_DIR, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting layers to train\n",
      "In model:  resnet_model\n",
      "In model:  fpn_model\n",
      "In model:  fpn_l1_model\n",
      "In model:  rpn_model\n",
      "Re-starting from epoch 240\n"
     ]
    }
   ],
   "source": [
    "# Load weights trained on Imagenet\n",
    "# model.load_weights('/gpfs01/bethge/home/cmichaelis/projects/2018-03_Siamese_Mask_RCNN/logs/imagenet20180511T1119/mask_rcnn_imagenet_0810.h5', by_name=True)\n",
    "# model.set_log_dir()\n",
    "model.set_trainable(\"(res4.*)|(bn4.*)|(res5.*)|(bn5.*)|(mrcnn\\_.*)|(rpn\\_.*)|(fpn\\_.*)\")\n",
    "# model.load_weights('/gpfs01/bethge/home/cmichaelis/projects/2018-03_Siamese_Mask_RCNN/logs/coco20180614T2205/mask_rcnn_coco_0236.h5', by_name=True)\n",
    "model.load_weights('logs/coco20180614T2205/mask_rcnn_coco_0240.h5', by_name=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 240. LR=0.001\n",
      "\n",
      "Checkpoint Path: /gpfs01/bethge/home/iustyuzh/siamese-mask-rcnn/logs/coco20180614T2205/mask_rcnn_coco_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "In model:  resnet_model\n",
      "In model:  fpn_model\n",
      "In model:  fpn_l1_model\n",
      "In model:  rpn_model\n",
      "\n",
      "Starting at epoch 240. LR=0.001\n",
      "\n",
      "Checkpoint Path: /gpfs01/bethge/home/iustyuzh/siamese-mask-rcnn/logs/coco20180614T2205/mask_rcnn_coco_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "In model:  resnet_model\n",
      "In model:  fpn_model\n",
      "In model:  fpn_l1_model\n",
      "In model:  rpn_model\n",
      "\n",
      "Starting at epoch 240. LR=0.001\n",
      "\n",
      "Checkpoint Path: /gpfs01/bethge/home/iustyuzh/siamese-mask-rcnn/logs/coco20180614T2205/mask_rcnn_coco_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "In model:  resnet_model\n",
      "In model:  fpn_model\n",
      "In model:  fpn_l1_model\n",
      "In model:  rpn_model\n",
      "Epoch 241/480\n",
      "1000/1000 [==============================] - 1772s 2s/step - loss: 0.9602 - rpn_class_loss: 0.0996 - rpn_bbox_loss: 0.0895 - mrcnn_class_loss: 0.2476 - mrcnn_bbox_loss: 0.1517 - mrcnn_mask_loss: 0.3711 - val_loss: 0.9721 - val_rpn_class_loss: 0.1043 - val_rpn_bbox_loss: 0.0889 - val_mrcnn_class_loss: 0.2537 - val_mrcnn_bbox_loss: 0.1602 - val_mrcnn_mask_loss: 0.3645\n",
      "Epoch 242/480\n",
      "1000/1000 [==============================] - 1601s 2s/step - loss: 0.9630 - rpn_class_loss: 0.0973 - rpn_bbox_loss: 0.0893 - mrcnn_class_loss: 0.2502 - mrcnn_bbox_loss: 0.1519 - mrcnn_mask_loss: 0.3737 - val_loss: 1.0160 - val_rpn_class_loss: 0.1096 - val_rpn_bbox_loss: 0.0845 - val_mrcnn_class_loss: 0.2765 - val_mrcnn_bbox_loss: 0.1704 - val_mrcnn_mask_loss: 0.3745\n",
      "Epoch 243/480\n",
      " 141/1000 [===>..........................] - ETA: 19:18 - loss: 0.9791 - rpn_class_loss: 0.1042 - rpn_bbox_loss: 0.0890 - mrcnn_class_loss: 0.2606 - mrcnn_bbox_loss: 0.1512 - mrcnn_mask_loss: 0.3736"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error processing image {'id': 7336, 'source': 'ade20k', 'path': '/gpfs01/bethge/data/ADE20K_2016_07_26/images/training/d/doorway/indoor/ADE_train_00007337.jpg', 'width': 2953, 'height': 2586, 'annotations': {'class_index': array([ 236,  530,  677,  774,  976,  978, 1180, 1247, 1395, 1910, 2420,\n",
      "       2676, 2684, 2850, 2855, 2932, 2978, 2982, 3055])}}\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-7-a1b8c9b77578>\", line 101, in siamese_data_generator\n",
      "    target = siamese_utils.get_one_target(category, dataset, config, augmentation=augmentation)\n",
      "  File \"/gpfs01/bethge/home/iustyuzh/siamese-mask-rcnn/utils.py\", line 48, in get_one_target\n",
      "    box_ind = np.random.choice(np.where(target_class_ids == category)[0])\n",
      "  File \"mtrand.pyx\", line 1126, in mtrand.RandomState.choice\n",
      "ValueError: a must be non-empty\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1542s 2s/step - loss: 0.9679 - rpn_class_loss: 0.0986 - rpn_bbox_loss: 0.0889 - mrcnn_class_loss: 0.2599 - mrcnn_bbox_loss: 0.1513 - mrcnn_mask_loss: 0.3687 - val_loss: 1.0129 - val_rpn_class_loss: 0.1118 - val_rpn_bbox_loss: 0.1111 - val_mrcnn_class_loss: 0.2418 - val_mrcnn_bbox_loss: 0.1656 - val_mrcnn_mask_loss: 0.3820\n",
      "Epoch 244/480\n",
      " 473/1000 [=============>................] - ETA: 12:58 - loss: 0.9670 - rpn_class_loss: 0.0987 - rpn_bbox_loss: 0.0972 - mrcnn_class_loss: 0.2476 - mrcnn_bbox_loss: 0.1512 - mrcnn_mask_loss: 0.3717"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error processing image {'id': 373476, 'source': 'coco', 'path': '/gpfs01/bethge/share/mscoco/COCO/train2017/000000373476.jpg', 'width': 500, 'height': 358, 'annotations': [{'segmentation': [[470.04, 251.64, 468.73, 238.86, 470.19, 228.84, 472.07, 225.65, 472.36, 217.22, 472.66, 213.74, 482.82, 214.17, 489.07, 214.46, 487.91, 220.42, 488.34, 225.06, 490.52, 232.62, 491.54, 237.7, 490.81, 244.67, 490.23, 250.63, 490.37, 251.64, 486.74, 253.82, 479.05, 254.4, 472.36, 253.82, 470.48, 253.1]], 'area': 771.8444499999994, 'iscrowd': 0, 'image_id': 373476, 'bbox': [468.73, 213.74, 22.81, 40.66], 'category_id': 44, 'id': 85973}, {'segmentation': [[118.27, 256.07, 121.33, 253.74, 127.12, 252.41, 134.17, 251.75, 144.67, 251.75, 153.58, 251.81, 159.43, 252.01, 163.96, 254.14, 163.29, 257.13, 162.09, 262.79, 159.63, 269.97, 155.84, 273.76, 151.72, 276.62, 143.61, 278.15, 133.03, 278.15, 126.45, 274.82, 121.86, 270.17, 119.87, 263.45, 118.27, 256.67]], 'area': 982.8483999999996, 'iscrowd': 0, 'image_id': 373476, 'bbox': [118.27, 251.75, 45.69, 26.4], 'category_id': 51, 'id': 710174}, {'segmentation': [[203.54, 160.76, 254.22, 160.76, 260.66, 102.84, 320.19, 102.04, 302.49, 157.55, 290.42, 172.83]], 'area': 3688.0024000000003, 'iscrowd': 0, 'image_id': 373476, 'bbox': [203.54, 102.04, 116.65, 70.79], 'category_id': 73, 'id': 1102590}, {'segmentation': [[279.16, 238.8, 275.94, 218.69, 349.96, 216.27, 378.92, 143.07, 401.44, 146.28, 372.48, 239.61, 277.55, 245.24]], 'area': 4388.805350000001, 'iscrowd': 0, 'image_id': 373476, 'bbox': [275.94, 143.07, 125.5, 102.17], 'category_id': 73, 'id': 1103847}, {'segmentation': [[298.47, 254.22, 404.66, 249.39, 434.43, 139.18, 481.89, 135.16, 451.32, 296.86, 302.49, 293.64]], 'area': 12613.556199999997, 'iscrowd': 0, 'image_id': 373476, 'bbox': [298.47, 135.16, 183.42, 161.7], 'category_id': 73, 'id': 1104314}, {'segmentation': [[335.62, 304.99, 341.12, 311.55, 355.03, 314.17, 390.44, 312.86, 403.56, 311.81, 409.59, 302.37, 407.23, 298.17, 396.74, 290.82, 394.38, 287.15, 390.18, 287.94, 382.57, 284.79, 368.93, 282.17, 354.77, 284.53, 344.54, 288.2, 338.76, 293.97]], 'area': 1800.4450000000008, 'iscrowd': 0, 'image_id': 373476, 'bbox': [335.62, 282.17, 73.97, 32.0], 'category_id': 74, 'id': 1107475}, {'segmentation': [[83.41, 80.95, 83.34, 82.68, 83.01, 85.07, 82.94, 87.52, 83.54, 87.72, 84.67, 85.4, 85.0, 84.07, 85.0, 83.01], [87.52, 84.93, 92.1, 91.83, 92.1, 93.76, 87.52, 97.6, 87.85, 93.49, 87.12, 91.1]], 'area': 42.626199999999926, 'iscrowd': 0, 'image_id': 373476, 'bbox': [82.94, 80.95, 9.16, 16.65], 'category_id': 77, 'id': 1638297}, {'segmentation': [[299.39, 160.15, 299.39, 150.09, 319.62, 150.65, 320.07, 161.49, 319.96, 162.05, 311.02, 158.58, 304.2, 158.47]], 'area': 186.21580000000026, 'iscrowd': 0, 'image_id': 373476, 'bbox': [299.39, 150.09, 20.68, 11.96], 'category_id': 47, 'id': 2098774}]}\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-7-a1b8c9b77578>\", line 101, in siamese_data_generator\n",
      "    target = siamese_utils.get_one_target(category, dataset, config, augmentation=augmentation)\n",
      "  File \"/gpfs01/bethge/home/iustyuzh/siamese-mask-rcnn/utils.py\", line 48, in get_one_target\n",
      "    box_ind = np.random.choice(np.where(target_class_ids == category)[0])\n",
      "  File \"mtrand.pyx\", line 1126, in mtrand.RandomState.choice\n",
      "ValueError: a must be non-empty\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1603s 2s/step - loss: 0.9588 - rpn_class_loss: 0.0965 - rpn_bbox_loss: 0.0906 - mrcnn_class_loss: 0.2498 - mrcnn_bbox_loss: 0.1507 - mrcnn_mask_loss: 0.3706 - val_loss: 0.9795 - val_rpn_class_loss: 0.1102 - val_rpn_bbox_loss: 0.0854 - val_mrcnn_class_loss: 0.2504 - val_mrcnn_bbox_loss: 0.1555 - val_mrcnn_mask_loss: 0.3775\n",
      "Epoch 245/480\n",
      "1000/1000 [==============================] - 1618s 2s/step - loss: 0.9645 - rpn_class_loss: 0.1001 - rpn_bbox_loss: 0.0896 - mrcnn_class_loss: 0.2517 - mrcnn_bbox_loss: 0.1517 - mrcnn_mask_loss: 0.3709 - val_loss: 0.9724 - val_rpn_class_loss: 0.1206 - val_rpn_bbox_loss: 0.0877 - val_mrcnn_class_loss: 0.2313 - val_mrcnn_bbox_loss: 0.1502 - val_mrcnn_mask_loss: 0.3820\n",
      "Epoch 246/480\n",
      " 230/1000 [=====>........................] - ETA: 17:37 - loss: 0.9530 - rpn_class_loss: 0.0978 - rpn_bbox_loss: 0.0867 - mrcnn_class_loss: 0.2395 - mrcnn_bbox_loss: 0.1519 - mrcnn_mask_loss: 0.3765"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error processing image {'id': 477655, 'source': 'coco', 'path': '/gpfs01/bethge/share/mscoco/COCO/train2017/000000477655.jpg', 'width': 640, 'height': 427, 'annotations': [{'segmentation': [[244.43, 400.16, 237.53, 392.35, 235.89, 381.07, 236.43, 371.98, 242.8, 362.53, 247.52, 356.35, 257.34, 351.44, 264.25, 350.17, 274.25, 349.8, 281.7, 352.53, 289.88, 357.99, 293.52, 363.26, 292.61, 369.44, 291.88, 376.53, 295.16, 377.98, 298.25, 377.98, 298.79, 381.26, 295.16, 392.16, 289.7, 402.16, 287.16, 402.71, 267.34, 401.8, 254.43, 400.89]], 'area': 2644.0787500000006, 'iscrowd': 0, 'image_id': 477655, 'bbox': [235.89, 349.8, 62.9, 52.91], 'category_id': 37, 'id': 303598}]}\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-7-a1b8c9b77578>\", line 101, in siamese_data_generator\n",
      "    target = siamese_utils.get_one_target(category, dataset, config, augmentation=augmentation)\n",
      "  File \"/gpfs01/bethge/home/iustyuzh/siamese-mask-rcnn/utils.py\", line 48, in get_one_target\n",
      "    box_ind = np.random.choice(np.where(target_class_ids == category)[0])\n",
      "  File \"mtrand.pyx\", line 1126, in mtrand.RandomState.choice\n",
      "ValueError: a must be non-empty\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1561s 2s/step - loss: 0.9525 - rpn_class_loss: 0.0980 - rpn_bbox_loss: 0.0897 - mrcnn_class_loss: 0.2414 - mrcnn_bbox_loss: 0.1517 - mrcnn_mask_loss: 0.3711 - val_loss: 0.9672 - val_rpn_class_loss: 0.1045 - val_rpn_bbox_loss: 0.1124 - val_mrcnn_class_loss: 0.2248 - val_mrcnn_bbox_loss: 0.1502 - val_mrcnn_mask_loss: 0.3748\n",
      "Epoch 247/480\n",
      "1000/1000 [==============================] - 1585s 2s/step - loss: 0.9675 - rpn_class_loss: 0.1021 - rpn_bbox_loss: 0.0886 - mrcnn_class_loss: 0.2557 - mrcnn_bbox_loss: 0.1530 - mrcnn_mask_loss: 0.3676 - val_loss: 0.9989 - val_rpn_class_loss: 0.1128 - val_rpn_bbox_loss: 0.0930 - val_mrcnn_class_loss: 0.2647 - val_mrcnn_bbox_loss: 0.1527 - val_mrcnn_mask_loss: 0.3751\n",
      "Epoch 248/480\n",
      "1000/1000 [==============================] - 1615s 2s/step - loss: 0.9697 - rpn_class_loss: 0.1017 - rpn_bbox_loss: 0.0918 - mrcnn_class_loss: 0.2550 - mrcnn_bbox_loss: 0.1511 - mrcnn_mask_loss: 0.3696 - val_loss: 0.9787 - val_rpn_class_loss: 0.1097 - val_rpn_bbox_loss: 0.0911 - val_mrcnn_class_loss: 0.2597 - val_mrcnn_bbox_loss: 0.1519 - val_mrcnn_mask_loss: 0.3656\n",
      "Epoch 249/480\n",
      " 437/1000 [============>.................] - ETA: 13:46 - loss: 0.9610 - rpn_class_loss: 0.1001 - rpn_bbox_loss: 0.0877 - mrcnn_class_loss: 0.2502 - mrcnn_bbox_loss: 0.1524 - mrcnn_mask_loss: 0.3699"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error processing image {'id': 188445, 'source': 'coco', 'path': '/gpfs01/bethge/share/mscoco/COCO/train2017/000000188445.jpg', 'width': 640, 'height': 427, 'annotations': [{'segmentation': [[138.73, 323.08, 139.74, 322.12, 141.18, 321.68, 142.19, 321.78, 143.58, 322.5, 144.11, 324.28, 143.78, 325.82, 142.91, 326.44, 141.61, 326.68, 139.31, 326.3, 138.73, 325.24, 138.54, 324.18, 138.83, 323.08]], 'area': 22.0708000000001, 'iscrowd': 0, 'image_id': 188445, 'bbox': [138.54, 321.68, 5.57, 5.0], 'category_id': 37, 'id': 302130}, {'segmentation': [[184.56, 308.77, 194.86, 290.66, 207.66, 272.24, 206.1, 269.12, 201.1, 273.8, 182.68, 308.14]], 'area': 130.29170000000045, 'iscrowd': 0, 'image_id': 188445, 'bbox': [182.68, 269.12, 24.98, 39.65], 'category_id': 39, 'id': 629524}]}\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-7-a1b8c9b77578>\", line 101, in siamese_data_generator\n",
      "    target = siamese_utils.get_one_target(category, dataset, config, augmentation=augmentation)\n",
      "  File \"/gpfs01/bethge/home/iustyuzh/siamese-mask-rcnn/utils.py\", line 48, in get_one_target\n",
      "    box_ind = np.random.choice(np.where(target_class_ids == category)[0])\n",
      "  File \"mtrand.pyx\", line 1126, in mtrand.RandomState.choice\n",
      "ValueError: a must be non-empty\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1575s 2s/step - loss: 0.9558 - rpn_class_loss: 0.0994 - rpn_bbox_loss: 0.0867 - mrcnn_class_loss: 0.2498 - mrcnn_bbox_loss: 0.1499 - mrcnn_mask_loss: 0.3694 - val_loss: 1.0362 - val_rpn_class_loss: 0.1215 - val_rpn_bbox_loss: 0.0945 - val_mrcnn_class_loss: 0.2664 - val_mrcnn_bbox_loss: 0.1665 - val_mrcnn_mask_loss: 0.3867\n",
      "Epoch 250/480\n",
      " 725/1000 [====================>.........] - ETA: 6:37 - loss: 0.9599 - rpn_class_loss: 0.0969 - rpn_bbox_loss: 0.0878 - mrcnn_class_loss: 0.2535 - mrcnn_bbox_loss: 0.1497 - mrcnn_mask_loss: 0.3714"
     ]
    }
   ],
   "source": [
    "model.train([coco_train, ade20k_train], [coco_val, ade20k_val], learning_rate=config.LEARNING_RATE, epochs=120, layers=\"heads\")\n",
    "model.config.LOSS_WEIGHTS = {'rpn_class_loss': 2.0, 'rpn_bbox_loss': 0.2/2, 'mrcnn_class_loss': 2.0, 'mrcnn_bbox_loss': 0.5, 'mrcnn_mask_loss': 1.0}\n",
    "model.train([coco_train, ade20k_train], [coco_val, ade20k_val], learning_rate=config.LEARNING_RATE, epochs=240, layers=\"4+\")\n",
    "# model.config.LOSS_WEIGHTS = {'rpn_class_loss': 2.0/2, 'rpn_bbox_loss': 0.2/2, 'mrcnn_class_loss': 2.0, 'mrcnn_bbox_loss': 0.5/2, 'mrcnn_mask_loss': 1.0/2}\n",
    "model.train([coco_train, ade20k_train], [coco_val, ade20k_val], learning_rate=config.LEARNING_RATE, epochs=480, layers=\"2+\")\n",
    "# model.config.LOSS_WEIGHTS = {'rpn_class_loss': 2.0/4, 'rpn_bbox_loss': 0.2/4, 'mrcnn_class_loss': 2.0, 'mrcnn_bbox_loss': 0.5/2, 'mrcnn_mask_loss': 1.0/2}\n",
    "model.train([coco_train, ade20k_train], [coco_val, ade20k_val], learning_rate=config.LEARNING_RATE, epochs=960, layers=\"all\")\n",
    "model.train([coco_train, ade20k_train], [coco_val, ade20k_val], learning_rate=config.LEARNING_RATE/10, epochs=1020, layers=\"all\")\n",
    "model.train([coco_train, ade20k_train], [coco_val, ade20k_val], learning_rate=config.LEARNING_RATE/100, epochs=1080, layers=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
