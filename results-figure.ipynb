{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "#%load_ext line_profiler\n",
    "\n",
    "import tensorflow as tf\n",
    "slim = tf.contrib.slim\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "sess_config = tf.ConfigProto()\n",
    "#config.gpu_options.allow_growth=True\n",
    "#config.gpu_options.per_process_gpu_memory_fraction=0.45\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "ADE20K_DATA = '/gpfs01/bethge/data/ADE20K_2016_07_26'\n",
    "COCO_DATA = '/gpfs01/bethge/share/mscoco/COCO'\n",
    "PASCAL_VOC_DATA = '/gpfs01/bethge/data/PascalVOC'\n",
    "MASK_RCNN_MODEL_PATH = 'Mask_RCNN/'\n",
    "SLIM_MODELS_PATH = 'slim/'\n",
    "TRANSFORMER_MODELS_PATH = 'transformer/'\n",
    "SIAMESE_MASK_RCNN_PATH = '/gpfs01/bethge/home/cmichaelis/projects/2018-03_Siamese_Mask_RCNN/siamese-mask-rcnn/'\n",
    "\n",
    "if MASK_RCNN_MODEL_PATH not in sys.path:\n",
    "    sys.path.append(MASK_RCNN_MODEL_PATH)\n",
    "if SIAMESE_MASK_RCNN_PATH not in sys.path:\n",
    "    sys.path.append(SIAMESE_MASK_RCNN_PATH)\n",
    "    \n",
    "from samples.coco import coco\n",
    "from samples.ade20k import ade20k\n",
    "from mrcnn import utils\n",
    "from mrcnn import model as modellib\n",
    "from mrcnn import visualize\n",
    "    \n",
    "if SLIM_MODELS_PATH not in sys.path:\n",
    "    sys.path.append(SLIM_MODELS_PATH)\n",
    "if TRANSFORMER_MODELS_PATH not in sys.path:\n",
    "    sys.path.append(TRANSFORMER_MODELS_PATH)\n",
    "    \n",
    "import utils as siamese_utils\n",
    "import model as siamese_model\n",
    "import config as siamese_config\n",
    "import siamese_mrcnn_models as model_zoo\n",
    "    \n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "import imgaug\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "# from spatial_transformer import transformer\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:98% !important; }</style>\"))\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.getcwd()\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nopascal_classes = [8,10,11,12,13,14,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,60,62,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pascal_classes = np.array(range(1,81))[np.array([i not in nopascal_classes for i in range(1,81)])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load COCO/val dataset\n",
    "val_dataset = siamese_utils.IndexedCocoDataset()\n",
    "val_dataset.load_coco(COCO_DATA, \"val\", year=2017)\n",
    "val_dataset.prepare()\n",
    "val_dataset.build_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainConfig(siamese_config.Config):\n",
    "    # Set batch size to 1 since we'll be running inference on\n",
    "    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    NUM_CLASSES = 1 + 1\n",
    "    NAME = 'coco'\n",
    "    EXPERIMENT = 'evaluation'\n",
    "    # Reduced image sizes\n",
    "    TARGET_MAX_DIM = 96\n",
    "    TARGET_MIN_DIM = 75\n",
    "    IMAGE_MIN_DIM = 400\n",
    "    IMAGE_MAX_DIM = 512\n",
    "    # Reduce model size\n",
    "    FPN_CLASSIF_FC_LAYERS_SIZE = 512\n",
    "    FPN_FEATUREMAPS = 256\n",
    "    # Reduce number of rois at all stages\n",
    "    RPN_ANCHOR_STRIDE = 1\n",
    "    RPN_TRAIN_ANCHORS_PER_IMAGE = 64\n",
    "    POST_NMS_ROIS_TRAINING = 500\n",
    "    POST_NMS_ROIS_INFERENCE = 500\n",
    "    TRAIN_ROIS_PER_IMAGE = 100\n",
    "    DETECTION_MAX_INSTANCES = 30\n",
    "    MAX_GT_INSTANCES = 30\n",
    "    # Adapt NMS Threshold\n",
    "    DETECTION_NMS_THRESHOLD = 0.5\n",
    "    # Adapt loss weights\n",
    "    LOSS_WEIGHTS = {'rpn_class_loss': 2.0, \n",
    "                    'rpn_bbox_loss': 0.1, \n",
    "                    'mrcnn_class_loss': 2.0, \n",
    "                    'mrcnn_bbox_loss': 0.5, \n",
    "                    'mrcnn_mask_loss': 1.0}\n",
    "    TRAIN_BN = False\n",
    "    \n",
    "config = TrainConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps 100, 200, 300, ...\n",
    "models = ['/gpfs01/bethge/home/cmichaelis/projects/2018-03_Siamese_Mask_RCNN/experiments/logs/coco_long/siamese_mrcnn_0310.h5']\n",
    "\n",
    "for i in range(len(models)):\n",
    "    # Create model object in inference mode.\n",
    "    model = siamese_model.SiameseMaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config)\n",
    "    \n",
    "    # set layers trainable for resnet weight loading\n",
    "    epoch_index = int(models[i][-7:-3])\n",
    "    if epoch_index <= 80:\n",
    "        model.set_trainable(\"(mrcnn\\_.*)|(rpn\\_.*)|(fpn\\_.*)\")\n",
    "    elif epoch_index <= 240:\n",
    "        model.set_trainable(\"(res4.*)|(bn4.*)|(res5.*)|(bn5.*)|(mrcnn\\_.*)|(rpn\\_.*)|(fpn\\_.*)\")          \n",
    "    else:\n",
    "        model.set_trainable(\".*\")\n",
    "    \n",
    "    # Load weights\n",
    "    model.load_weights(models[i], by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []\n",
    "images = []\n",
    "rois = []\n",
    "masks = []\n",
    "class_ids = []\n",
    "scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIR = '/gpfs01/bethge/home/cmichaelis/datasets/COCO/val2017/'\n",
    "\n",
    "# Load a random image from the images folder\n",
    "# file_names = next(os.walk(IMAGE_DIR))[2]\n",
    "# image = skimage.io.imread(os.path.join(IMAGE_DIR, random.choice(file_names)))\n",
    "coco_image_ids = [val_dataset.image_info[id][\"id\"] for id in val_dataset.image_ids]\n",
    "\n",
    "p = 1\n",
    "while p < 2:\n",
    "\n",
    "    # Load target\n",
    "    category = 3\n",
    "    random_image_id = np.random.choice(val_dataset.category_image_index[category])\n",
    "    # random_image_id = 4005\n",
    "    image = val_dataset.load_image(random_image_id)\n",
    "\n",
    "    if np.abs(image.shape[0] / image.shape[1] - (3 / 2)) > 0.01:\n",
    "        continue\n",
    "\n",
    "#     target_image, _, target_class_ids, target_boxes, _ = \\\n",
    "#         modellib.load_image_gt(val_dataset, config, random_image_id, augmentation=False,\n",
    "#                       use_mini_mask=config.USE_MINI_MASK)\n",
    "\n",
    "#     box_ind = np.random.choice(np.where(target_class_ids == category)[0])   \n",
    "#     tb = target_boxes[box_ind,:]\n",
    "#     target = target_image[tb[0]:tb[2],tb[1]:tb[3],:]\n",
    "    target1, window, scale, padding1, crop = siamese_utils.get_one_target(category, val_dataset, config, return_all=True)\n",
    "\n",
    "    # Run detection\n",
    "    results1 = model.detect([target1], [image], verbose=1)\n",
    "    r1 = results1[0]\n",
    "    cropped_target1 = siamese_utils.crop_target(target1, padding1)\n",
    "    rois1 = r1['rois']\n",
    "    masks1 = r1['masks']\n",
    "    class_ids1 = r1['class_ids']\n",
    "    scores1 = r1['scores']\n",
    "    \n",
    "    target2, window, scale, padding2, crop = siamese_utils.get_one_target(category, val_dataset, config, return_all=True)\n",
    "\n",
    "    # Run detection\n",
    "    results2 = model.detect([target2], [image], verbose=1)\n",
    "    r2 = results2[0]\n",
    "    cropped_target2 = siamese_utils.crop_target(target2, padding2)\n",
    "    rois2 = r2['rois']\n",
    "    masks2 = r2['masks']\n",
    "    class_ids2 = r2['class_ids']\n",
    "    scores2 = r2['scores']\n",
    "\n",
    "#     cropped_target = siamese_utils.crop_target(target, padding)\n",
    "#     siamese_utils.display_siamese_instances2(cropped_target, image, r['rois'], r['masks'], r['class_ids'], r['scores'])\n",
    "\n",
    "#     targets.append(cropped_target)\n",
    "#     images.append(image)\n",
    "#     rois.append(r['rois'])\n",
    "#     masks.append(r['masks'])\n",
    "#     class_ids.append(r['class_ids'])\n",
    "#     scores.append(r['scores'])\n",
    "    \n",
    "    p = p + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_utils.display_siamese_instances_teaser([cropped_target1, cropped_target2],\n",
    "                                               [image]*2,\n",
    "                                               [rois1, rois2],\n",
    "                                               [masks1, masks2],\n",
    "                                               [class_ids1, class_ids2],\n",
    "                                               [scores1, scores2],\n",
    "                                               figsize=(8 + 2 + 8 + 2 + 8, 12), fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []\n",
    "images = []\n",
    "rois = []\n",
    "masks = []\n",
    "class_ids = []\n",
    "scores = []\n",
    "\n",
    "p = 0\n",
    "while p < 16:\n",
    "    # Load target\n",
    "    category = np.random.choice(list(range(1,81)))\n",
    "    random_image_id = np.random.choice(val_dataset.category_image_index[category])\n",
    "    # random_image_id = 4005\n",
    "    image = val_dataset.load_image(random_image_id)\n",
    "\n",
    "    if np.abs(image.shape[0] / image.shape[1] - (2 / 3)) > 0.1:\n",
    "        continue\n",
    "    \n",
    "    target, window, scale, padding, crop = siamese_utils.get_one_target(category, val_dataset, config, return_all=True, target_size_limit=20)\n",
    "\n",
    "    # Run detection\n",
    "    results = model.detect([target], [image], verbose=0)\n",
    "    r = results[0]\n",
    "    cropped_target = siamese_utils.crop_target(target, padding)\n",
    "    \n",
    "    targets.append(cropped_target)\n",
    "    images.append(image)\n",
    "    rois.append(r['rois'])\n",
    "    masks.append(r['masks'])\n",
    "    class_ids.append(r['class_ids'])\n",
    "    scores.append(r['scores'])\n",
    "    \n",
    "    p = p+1\n",
    "    \n",
    "siamese_utils.display_siamese_instances2(targets, images, rois, masks, class_ids, scores, figsize=(16, 16 * 2 / 3),\n",
    "                                         target_shift=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_utils.display_siamese_instances2(targets, images, rois, masks, class_ids, scores, figsize=(16, 16 * 2 / 3),\n",
    "                                         target_shift=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_utils.display_siamese_instances2(targets, images, rois, masks, class_ids, scores, figsize=(16, 16 * 2 / 3),\n",
    "                                         target_shift=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4872, 4005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_image_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_utils.display_siamese_instances2(target[:, 28:-28,:], image, r['rois'], r['masks'], r['class_ids'], r['scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target[slice(21, 96 - 21), slice(0, 96),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(target[slice(21, 96 - 21), slice(0, 96),:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
